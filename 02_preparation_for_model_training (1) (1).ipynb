{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddca83a8",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b113e10-1b6d-449b-b712-f79f3f610ef5",
   "metadata": {},
   "source": [
    "# Preparation for Model Training #\n",
    "We noticed some issues with the TrafficCamNet model for our video AI application. It's likely that the model wasn't trained for our exact parking garage use case. For the remaining of the lab, we will use the TAO Toolkit to fine-tune the model so that it can adapt to our environment. Below is what a typical model development workflow looks like. We start by preparing a pre-trained model and the data. Next, we prepare the configuration file(s) and begin to train the model with new data and evaluate its performance. We export the model once its satisfactory. Note that this does not include inference optimization steps, which is very important for video AI applications that are deployed on edge devices. \n",
    "<p><img src='images/pre-trained_model_workflow.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1934605-75a4-49fa-9a5d-214f4a001400",
   "metadata": {},
   "source": [
    "## Learning Objectives ##\n",
    "In this notebook, you will learn how to prepare for training a video AI model using the TAO Toolkit, including: \n",
    "* Understanding Model Specification\n",
    "* Preparing Data for TAO Toolkit Consumption\n",
    "* Editing Spec Files for TAO Toolkit Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756103b-e433-42e4-a9ec-9245d6a64b51",
   "metadata": {},
   "source": [
    "**Table of Contents**<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Detectnet_v2 Object Detection Model](#s1)\n",
    "    * [Directory Structure](#s1.1)\n",
    "    * [Model Objective](#s1.2)\n",
    "2. [Prepare Pre-trained Model](#s2)\n",
    "    * [Exercise #1 - Review Model Card](#e1)\n",
    "3. [Prepare Data Set](#s3)\n",
    "    * [Annotation](#s3.1)\n",
    "    * [Exploratory Data Analysis](#s3.2)\n",
    "    * [Covert Video File into Frame Images](#s3.3)\n",
    "    * [Generate Labels](#s3.4)\n",
    "    * [Converting to TFRecords](#s3.5)\n",
    "    * [Exercise #2 Dataset Convert](#e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fcd16-3d29-405d-b032-6b48226dad63",
   "metadata": {},
   "source": [
    "<a name='s1'></a>\n",
    "## DetectNet_v2 Object Detection Model ##\n",
    "As we previously learned, the [TrafficCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/models/tlt_trafficcamnet) purpose-built model is based on NVIDIA DetectNet_v2 detector with ResNet18 as a feature extractor. As such, we will use the `detectnet_v2` task, which supports the following subtasks: \n",
    "* `dataset_convert`\n",
    "* `train`\n",
    "* `evaluate`\n",
    "* `inference`\n",
    "* `prune`\n",
    "* `calibration_tensorfile`\n",
    "* `export`\n",
    "\n",
    "<p><img src='images/rewind.png' width=720><p>\n",
    "    \n",
    "These subtasks can be invoked using the convention `detectnet_v2 <subtask> <args_per_subtask>` on the command-line. Additionally, we can always find more information about these subtasks with `detector_v2 <subtask> --help`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259d244-84ef-44ff-a389-d4a6eeab7571",
   "metadata": {},
   "source": [
    "<a name='s1.1'></a>\n",
    "### Directory Structure ###\n",
    "We will use the below structure for our project, where the `tao_project` directory will hold most of the assets related to model training and outputs. \n",
    "\n",
    "<p><img src='images/project_structure.png' width=740></p>\n",
    "\n",
    "* The current directory is `/dli/task`. When using paths, it is most reliable to use the absolute path that begins with `/dli/task` as some of the functions will otherwise try to reference the paths relative to where they are called. \n",
    "* The higher level `data` directory represents the raw video data, vs. the lower level `tao_project/data` directory represents the preprocessed data to be used for model training. \n",
    "* The higher level `images` directory contains graphics used in this course and are not related to the video AI model. \n",
    "* The `spec_files` directory holds spec files that will be used for TAO Toolkit model training as well as DeepStream `Gst-nvinfer` plugin configuration files. \n",
    "* The `tao_project/models` directory will hold different versions of the model as we work to arrive at a final, optimized, product. Each folder will hold the corresponding model file (e.g. `.tlt` or `.etlt`), as well as accompanied assets such as `labels.txt`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2370d9e-d605-46ab-b474-2db51ef29635",
   "metadata": {},
   "source": [
    "Execute the below cell to set and create directories for the TAO Toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e62b5c30-81b7-4a34-9789-b59614f44d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/dli/task/tao_project’: File exists\n",
      "mkdir: cannot create directory ‘/dli/task/tao_project/data’: File exists\n",
      "mkdir: cannot create directory ‘/dli/task/tao_project/models’: File exists\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Set and create directories for the TAO Toolkit experiment\n",
    "import os\n",
    "\n",
    "os.environ['PROJECT_DIR']='/dli/task/tao_project'\n",
    "os.environ['SOURCE_DATA_DIR']='/dli/task/data'\n",
    "os.environ['DATA_DIR']='/dli/task/tao_project/data'\n",
    "os.environ['MODELS_DIR']='/dli/task/tao_project/models'\n",
    "os.environ['SPEC_FILES_DIR']='/dli/task/spec_files'\n",
    "\n",
    "!mkdir $PROJECT_DIR\n",
    "!mkdir $DATA_DIR\n",
    "!mkdir $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4977de-9671-40ec-8324-d05803074c43",
   "metadata": {},
   "source": [
    "<a name='s1.2'></a>\n",
    "### Model Objective ###\n",
    "For our video AI application, we want to train a model that uses the TrafficCamNet as the starting point and provide it with additional (labeled) data so it can adapt to our specific camera angle, lighting condition, and other environmental conditions. We will be using the **unpruned pre-trained TrafficCamNet purpose-built model** as the starting point and training a custom **one-class Object Detection model** that is adapted to our use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a15e30",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2'></a>\n",
    "## Prepare Pre-trained Model and Data Set ##\n",
    "Developers typically begin by choosing and downloading a pre-trained model from [NGC](https://ngc.nvidia.com/) - either a highly accurate purpose-built model or just the pre-trained weights of the architecture of their choice. It's difficult to immediately identify which model/architecture will work best for a specific use case as there is often a tradeoff between time to train, accuracy, and inference performance. It is common to compare across multiple models before picking the best candidate.\n",
    "\n",
    "Here are some pointers that will help choose an appropriate model: \n",
    "* Look at the model inputs/outputs to decide if it will fit your use case. \n",
    "* Input format is also an important consideration. For example, TrafficCamNet, as well as other DetectNet_v2 models, expect the input to be 0-1 normalized with input channels in RGB order. Models that use a BGR order will require input preprocessing/mean subtraction that might result in suboptimal performance. \n",
    "\n",
    "We can use the `ngc registry model list <model_glob_string>` command to get a list of models that are hosted in the NGC model registry. For example, we can use `ngc registry model list nvidia/tao/*` to list all available models. The `--column` option identifies the columns of interest. More information about the NGC Registry CLI can be found in the [User Guide](https://docs.nvidia.com/dgx/pdf/ngc-registry-cli-user-guide.pdf). For each model, there is a pruned version that can be deployed as is or an unpruned version which can be used to re-train with more data for specific use cases. We will use the unpruned version as a start for training purposes. The `ngc registry model download-version <org>/[<team>/]<model-name:version>` command will download the model from the registry. It has a `--dest` option to specify the path to download directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8730b58",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1 - Review Model Cards ####\n",
    "Let's download a pre-trained model. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Review the model cards for [TrafficCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/models/tlt_trafficcamnet) and/or [DetectNet_v2](https://catalog.ngc.nvidia.com/orgs/nvidia/models/tlt_pretrained_detectnet_v2) models to understand where you can find important model specifications. \n",
    "* Execute the below cell to download the NGC CLI. \n",
    "* Execute the following cell to list all available models. \n",
    "* Execute the following cell to download the TrafficCamNet model. \n",
    "* Execute the following cell to check if the model has been downloaded. \n",
    "* Execute the following cell to create `labels.txt` if it doesn't exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "88994507",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLI=ngccli_cat_linux.zip\n",
      "--2022-08-29 18:52:51--  https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip\n",
      "Resolving ngc.nvidia.com (ngc.nvidia.com)... 18.165.83.53, 18.165.83.59, 18.165.83.111, ...\n",
      "Connecting to ngc.nvidia.com (ngc.nvidia.com)|18.165.83.53|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34014460 (32M) [application/zip]\n",
      "Saving to: ‘ngc_assets/ngccli/ngccli_cat_linux.zip’\n",
      "\n",
      "ngccli_cat_linux.zi 100%[===================>]  32.44M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-08-29 18:52:51 (225 MB/s) - ‘ngc_assets/ngccli/ngccli_cat_linux.zip’ saved [34014460/34014460]\n",
      "\n",
      "Archive:  ngc_assets/ngccli/ngccli_cat_linux.zip\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the NGC CLI\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p ngc_assets/ngccli\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P ngc_assets/ngccli\n",
    "!unzip -u \"ngc_assets/ngccli/$CLI\" \\\n",
    "       -d ngc_assets/ngccli/\n",
    "!rm ngc_assets/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(\"ngc_assets\", os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f0d1394b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+-------------------------+\n",
      "| Name                    | Repository              | Application             |\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Mandarin                | _zh_cn_conformer        |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | Punctuation and         |\n",
      "| Capitalization for      | apitalization_es_us_ber | Capitalization          |\n",
      "| Spanish                 | t_base                  |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | Punctuation and         |\n",
      "| Capitalization for      | apitalization_de_de_ber | Capitalization          |\n",
      "| German                  | t_base                  |                         |\n",
      "| TAO Pretrained          | nvidia/tao/pretrained_e | Object Detection        |\n",
      "| EfficientDet            | fficientdet             |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "| Capitalization for      | apitalization_hi_in_ber |                         |\n",
      "| Hindi                   | t_base                  |                         |\n",
      "| LPDNet                  | nvidia/tao/lpdnet       | Object Detection        |\n",
      "| Riva ASR English(en-US) | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| LM                      | _en_us_lm               |                         |\n",
      "| FaceDetectIR            | nvidia/tao/facedetectir | Object Detection        |\n",
      "| Riva ASR Spanish        | nvidia/tao/inverse_norm | NVIDIA Riva             |\n",
      "| Inverse Normalization   | alization_es_us         |                         |\n",
      "| Grammar                 |                         |                         |\n",
      "| Speech Synthesis HiFi-  | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| GAN                     | sis_hifigan             |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| English(en-US)          | _en_us_conformer        |                         |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| English                 | _en_us_citrinet         |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| French                  | _fr_fr_conformer        |                         |\n",
      "| Question Answering      | nvidia/tao/questionansw | Question Answering      |\n",
      "| SQUAD2.0 Megatron       | ering_squad_english_meg |                         |\n",
      "|                         | atron                   |                         |\n",
      "| PeopleSegNet            | nvidia/tao/peoplesegnet | Instance Segmentation   |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Spanish                 | _es_us_citrinet         |                         |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| German                  | _de_de_citrinet         |                         |\n",
      "| FaceDetect              | nvidia/tao/facenet      | Object Detection        |\n",
      "| Pose Classification     | nvidia/tao/poseclassifi | Pose Classification     |\n",
      "|                         | cationnet               |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| German                  | _de_de_conformer        |                         |\n",
      "| TAO Pretrained Instance | nvidia/tao/pretrained_i | Instance Segmentation   |\n",
      "| Segmentation            | nstance_segmentation    |                         |\n",
      "| Riva ASR German LM      | nvidia/tao/speechtotext | NVIDIA Riva EA          |\n",
      "|                         | _de_de_lm               |                         |\n",
      "| Speech Synthesis        | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| English FastPitch       | sis_english_fastpitch   |                         |\n",
      "| BodyPoseNet             | nvidia/tao/bodyposenet  | OTHER                   |\n",
      "| Riva ASR Hindi LM       | nvidia/tao/speechtotext | Speech To Text          |\n",
      "|                         | _hi_in_lm               |                         |\n",
      "| VehicleMakeNet          | nvidia/tao/vehiclemaken | Classification          |\n",
      "|                         | et                      |                         |\n",
      "| RIVA Citrinet ASR Hindi | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "| (hi-IN) - ASR set 1.0   | _hi_in_citrinet         |                         |\n",
      "| Riva ASR English(en-GB) | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "| LM                      | _en_gb_lm               |                         |\n",
      "| TrafficCamNet           | nvidia/tao/trafficcamne | Object Detection        |\n",
      "|                         | t                       |                         |\n",
      "| GestureNet              | nvidia/tao/gesturenet   | Gesture Classification  |\n",
      "| Gaze Estimation         | nvidia/tao/gazenet      | Gaze Detection          |\n",
      "| Riva ASR French LM      | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "|                         | _fr_fr_lm               |                         |\n",
      "| Riva ASR Mandarin LM    | nvidia/tao/speechtotext | Speech To Text          |\n",
      "|                         | _zh_cn_lm               |                         |\n",
      "| Question Answering      | nvidia/tao/questionansw | Question Answering      |\n",
      "| SQUAD2.0 Bert - Large   | ering_squad_english_ber |                         |\n",
      "|                         | tlarge                  |                         |\n",
      "| License Plate           | nvidia/tao/lprnet       | Character Recognition   |\n",
      "| Recognition             |                         |                         |\n",
      "| Speech Synthesis        | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| English Tacotron2       | sis_english_tacotron2   |                         |\n",
      "| HeartRateNet            | nvidia/tao/heartratenet | HeartRateNet Estimation |\n",
      "| Domain Classification   | nvidia/tao/domainclassi | Domain Classification   |\n",
      "| English Bert            | fication_english_bert   |                         |\n",
      "| Riva ASR German Inverse | nvidia/tao/inverse_norm | NVIDIA Riva             |\n",
      "| Normalization Grammar   | alization_de_de         |                         |\n",
      "| TAO Pretrained          | nvidia/tao/pretrained_d | Object Detection        |\n",
      "| DetectNet V2            | etectnet_v2             |                         |\n",
      "| RIVA Quartznet ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| English                 | _en_us_quartznet        |                         |\n",
      "| Joint Intent and Slot   | nvidia/tao/intentslotcl | Joint Intent And Slot   |\n",
      "| Classification          | assification_misty_engl | Classification          |\n",
      "| DistilBert              | ish_distilbert          |                         |\n",
      "| Named Entity            | nvidia/tao/namedentityr | Named Entity            |\n",
      "| Recognition Bert        | ecognition_english_bert | Recognition             |\n",
      "| RIVA English Fastpitch  | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| Female 1                | sis_en_us_fastpitch_fem |                         |\n",
      "|                         | ale_1                   |                         |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Mandarin                | _zh_cn_citrinet         |                         |\n",
      "| RIVA Citrinet ASR       | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Russian                 | _ru_ru_citrinet         |                         |\n",
      "| RIVA Citrinet 256 ASR   | nvidia/tao/speechtotext | Speech To Text          |\n",
      "| English                 | _en_us_citrinet256      |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| English                 | _en_gb_conformer        |                         |\n",
      "| Punctuation and         | nvidia/tao/punctuationc | Punctuation and         |\n",
      "| Capitalization Bert     | apitalization_english_b | Capitalization          |\n",
      "|                         | ert                     |                         |\n",
      "| RIVA English Fastpitch  | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| Male 1                  | sis_en_us_fastpitch_mal |                         |\n",
      "|                         | e_1                     |                         |\n",
      "| PeopleNet               | nvidia/tao/peoplenet    | Object Detection        |\n",
      "| Riva ASR Spanish LM     | nvidia/tao/speechtotext | NVIDIA Riva EA          |\n",
      "|                         | _es_us_lm               |                         |\n",
      "| RIVA Jasper ASR English | nvidia/tao/speechtotext | Speech to Text          |\n",
      "|                         | _en_us_jasper           |                         |\n",
      "| Riva TTS English US     | nvidia/tao/speechsynthe | NVIDIA Riva EA          |\n",
      "| Auxiliary Files         | sis_en_us_auxiliary_fil |                         |\n",
      "|                         | es                      |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | NVIDIA Riva             |\n",
      "| Hindi - ASR set 2.0     | _hi_in_conformer        |                         |\n",
      "| Riva ASR English        | nvidia/tao/inverse_norm | NVIDIA Riva             |\n",
      "| Inverse Normalization   | alization_en_us         |                         |\n",
      "| Grammar                 |                         |                         |\n",
      "| Riva TTS English        | nvidia/tao/normalizatio | NVIDIA Riva             |\n",
      "| Normalization Grammar   | n_en_us                 |                         |\n",
      "| Speech to Text English  | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| QuartzNet               | _english_quartznet      |                         |\n",
      "| Joint Intent and Slot   | nvidia/tao/intentslotcl | Joint Intent and Slot   |\n",
      "| Classification Bert     | assification_weather_en | classification          |\n",
      "|                         | glish_bert              |                         |\n",
      "| RIVA Hifigan Female 1   | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "|                         | sis_en_us_hifigan_femal |                         |\n",
      "|                         | e_1                     |                         |\n",
      "| RIVA Punctuation and    | nvidia/tao/punctuationc | NVIDIA Riva             |\n",
      "| Capitalization for      | apitalization_fr_fr_ber |                         |\n",
      "| French                  | t_base                  |                         |\n",
      "| Riva ASR English LM     | nvidia/tao/speechtotext | Riva                    |\n",
      "|                         | _english_lm             |                         |\n",
      "| TAO Pretrained Object   | nvidia/tao/pretrained_o | Object Detection        |\n",
      "| Detection               | bject_detection         |                         |\n",
      "| TAO Pretrained Semantic | nvidia/tao/pretrained_s | Semantic Segmentation   |\n",
      "| Segmentation            | emantic_segmentation    |                         |\n",
      "| Speech Synthesis        | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "| Waveglow                | sis_waveglow            |                         |\n",
      "| VehicleTypeNet          | nvidia/tao/vehicletypen | Classification          |\n",
      "|                         | et                      |                         |\n",
      "| PeopleSemSegnet         | nvidia/tao/peoplesemseg | OTHER                   |\n",
      "|                         | net                     |                         |\n",
      "| Facial Landmarks        | nvidia/tao/fpenet       | Fiducial Landmarks      |\n",
      "| Estimation              |                         |                         |\n",
      "| Speech to Text English  | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Citrinet                | _english_citrinet       |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Spanish                 | _es_us_conformer        |                         |\n",
      "| Question Answering      | nvidia/tao/questionansw | Question Answering      |\n",
      "| SQUAD2.0 Bert           | ering_squad_english_ber |                         |\n",
      "|                         | t                       |                         |\n",
      "| RIVA Conformer ASR      | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Russian - ASR set 1.0   | _ru_ru_conformer        |                         |\n",
      "| Action Recognition Net  | nvidia/tao/actionrecogn | Action Recognition      |\n",
      "|                         | itionnet                |                         |\n",
      "| Riva ASR Russian LM     | nvidia/tao/speechtotext | NVIDIA Riva EA          |\n",
      "|                         | _ru_ru_lm               |                         |\n",
      "| RIVA Hifigan Male 1     | nvidia/tao/speechsynthe | Text to Speech          |\n",
      "|                         | sis_en_us_hifigan_male_ |                         |\n",
      "|                         | 1                       |                         |\n",
      "| EmotionNet              | nvidia/tao/emotionnet   | Emotion Classification  |\n",
      "| Speech to Text English  | nvidia/tao/speechtotext | Speech to Text          |\n",
      "| Jasper                  | _english_jasper         |                         |\n",
      "| RIVA Punctuation        | nvidia/tao/punctuationc | Punctuation and         |\n",
      "|                         | apitalization_en_us_ber | Capitalization          |\n",
      "|                         | t_base                  |                         |\n",
      "| BodyPose3DNet           | nvidia/tao/bodypose3dne | Pose Estimation         |\n",
      "|                         | t                       |                         |\n",
      "| PointPillarNet          | nvidia/tao/pointpillarn | Object Detection        |\n",
      "|                         | et                      |                         |\n",
      "| DashCamNet              | nvidia/tao/dashcamnet   | Object Detection        |\n",
      "| Joint Intent and Slot   | nvidia/tao/intentslotcl | NLP                     |\n",
      "| Classification Misty    | assification_misty_engl |                         |\n",
      "| Bert                    | ish_bert                |                         |\n",
      "| TAO Pretrained          | nvidia/tao/pretrained_c | Classification          |\n",
      "| Classification          | lassification           |                         |\n",
      "+-------------------------+-------------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# List all available models\n",
    "!ngc registry model list nvidia/tao/* --column name --column repository --column application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "98ab3f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 85.32 MB in 6s, Download speed: 14.2 MB/s                \n",
      "--------------------------------------------------------------------------------\n",
      "   Transfer id: peoplenet_vunpruned_v1.0\n",
      "   Download status: Completed\n",
      "   Downloaded local path: /dli/task/tao_project/models/peoplenet_vunpruned_v1-1.0\n",
      "   Total files downloaded: 1\n",
      "   Total downloaded size: 85.32 MB\n",
      "   Started at: 2022-08-29 18:53:11.118272\n",
      "   Completed at: 2022-08-29 18:53:17.129269\n",
      "   Duration taken: 6s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the unpruned pre-trained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/peoplenet:unpruned_v1.0 \\\n",
    "    --dest $MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3d8a7ac2-cf8e-4536-98a0-066dea725ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 11.3 MB in 3s, Download speed: 3.77 MB/s               \n",
      "-------------------------------------------------------------------------------\n",
      "   Transfer id: peoplenet_vpruned_v1.0\n",
      "   Download status: Completed\n",
      "   Downloaded local path: /dli/task/tao_project/models/peoplenet_vpruned_v1-1.0\n",
      "   Total files downloaded: 2\n",
      "   Total downloaded size: 11.3 MB\n",
      "   Started at: 2022-08-29 18:53:24.281060\n",
      "   Completed at: 2022-08-29 18:53:27.285857\n",
      "   Duration taken: 3s\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Download the pruned pre-trained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/peoplenet:pruned_v1.0 \\\n",
    "    --dest $MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c03eafe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwx------ 2 root root 4096 Aug 29 18:39 peoplenet_vunpruned_v1.0\n",
      "drwx------ 2 root root 4096 Aug 29 18:39 peoplenet_vpruned_v1.0\n",
      "drwx------ 2 root root 4096 Aug 29 18:53 peoplenet_vunpruned_v1-1.0\n",
      "drwx------ 2 root root 4096 Aug 29 18:53 peoplenet_vpruned_v1-1.0\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check if models have been downloaded into directory\n",
    "!ls -rlt $MODELS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb57ff2-2960-4dc2-80e0-7ebae65debc8",
   "metadata": {},
   "source": [
    "**Observations**:<br>\n",
    "Below are some fields that are important to note: \n",
    "\n",
    "<p><img src='images/model_card_tao.png' width=1080></p>\n",
    "\n",
    "<p><img src='images/encryption_key.png' width=540></p>\n",
    "\n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "_Note that we're using the purpose-built TrafficCamNet model as the starting point for scene adaptation. Feel free to try the lab with other model architectures if there is time left at the end of the course. When working with purpose-built models from NGC, the correct **encryption key** is required to load the model. Users will be able to define their own export encryption key when training from a general purpose model. This is to protect proprietary IP and used to decrypt the `.etlt` model in DeepStream applications._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a21c3",
   "metadata": {},
   "source": [
    "<a name='s3'></a>\n",
    "## Prepare Data Set ##\n",
    "We're going to use annotated video data shot from the same camera at the NVIDIA headquarters parking lot for our model. It's important to recognize that the data that we're providing is limited and insufficient for training a model from scratch. Using the TAO Toolkit and transfer learning, we can use the TrafficCamNet model as the starting point and train a custom model. This is a common case when we can leverage TAO Toolkit's scene/domain adaptation capabilities. \n",
    "\n",
    "The TAO Toolkit requires the data to be in a specific format for training and evaluation: \n",
    "* The object detection tasks in the TAO Toolkit expects data in the `KITTI format`. \n",
    "    * The `images` directory contains the images to train on. \n",
    "    * The `labels` directory contains labels to the corresponding images. \n",
    "    * The `kitti_seq_to_map.json` file is _optional_ and contains a sequence to frame ID mapping for the frames in the images directory. It is useful if the data needs to be split into folds sequence wise. \n",
    "\n",
    "<p><img src='images/detection_input.png' width=720></p>\n",
    "\n",
    "* For comparison, the classification tasks expects a directory of images with the following structure, where each class has its own directory with the class name. \n",
    "\n",
    "<p><img src='images/classification_input.png' width=720></p>\n",
    "\n",
    "_You can find more details on data annotation format in the [TAO Toolkit User Guide](https://docs.nvidia.com/metropolis/TLT/tlt-user-guide/text/data_annotation_format.html#object-detection-kitti-format)_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9dc917",
   "metadata": {},
   "source": [
    "<a name='s3.1'></a>\n",
    "### Annotation ###\n",
    "Let's preview the video before moving forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1e8b9-ccf9-4285-aa2f-46135f658087",
   "metadata": {},
   "source": [
    "Execute the below cell to view the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "600ad2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data/VID-20220829-WA0030.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the video\n",
    "from IPython.display import Video\n",
    "\n",
    "Video(\"data/VID-20220829-WA0021.mp4\", width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e79388",
   "metadata": {},
   "source": [
    "In addition to video feeds, we need labels to evaluate the inference results (compare the actual objects to those detected by our deep learning models), and also to expand the ground truth for training purposes. This is normally a time consuming process, but this requirement is significantly reduced with transfer learning. There are a number of annotation tools that are publically available for use. Our data set annotation was generated (manually) using [Vatic](https://github.com/cvondrick/vatic) and provided in JSON format. Each entry starts with the `track_id`, representing a unique index for each person within the recording. The `track_id` provides a set of bounding boxes and their respective bounding box positions. Below, you can see elements of the annotation format:\n",
    "\n",
    "<p><img src=\"images/vatic.jpg\" width=720></p>\n",
    "\n",
    "This is a snapshot of the JSON file for the video. We're mainly interested in the bounding box coordinates captured for our object detection model: \n",
    "* **xbr**: an integer in the range between [0, frame width], indicating the right-most location of the bounding box in coordinates relative to the frame size.<br />\n",
    "* **xtl**: an integer in the range between [0, frame width], indicating the left-most location of the bounding box in coordinates relative to the frame size.<br />\n",
    "* **ybr**: an integer in the range between [0, frame height], indicating the bottom-most location of the bounding box in coordinates relative to the frame size.<br />\n",
    "* **ytl**: an integer in the range between [0, frame height], indicating the top-most location of the bounding box in coordinates relative to the frame size.<br />\n",
    "<p><img src=\"images/json_structure.png\" width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20e908",
   "metadata": {},
   "source": [
    "Execute the cell below to preview the annotation in JSON format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1d42767e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"0\": {\"boxes\": {\"0\": {\"occluded\": 0, \"ybr\": 27, \"ytl\": 15, \"xbr\": 310, \"outside\": 1, \"attributes\": [], \"xtl\": 290}, \"1\": {\"occluded\": 0, \"ybr\": 26, \"ytl\": 14, \"xbr\": 309, \"outside\": 1, \"attributes\": [], \"xtl\": 289}, \"2\": {\"occluded\": 0, \"ybr\": 26, \"ytl\": 14, \"xbr\": 309, \"outside\": 1, \"attributes\": [], \"xtl\": 289}, \"3\": {\"occluded\": 0, \"ybr\": 26, \"ytl\": 14, \"xbr\": 309, \"outside\": 1, \"attributes\": [], \"xtl\": 289}, \"4\": {\"occluded\": 0, \"ybr\": 26, \"ytl\": 14, \"xbr\": 308, \"outside\": 1, \"attributes\": [], \"xtl\": 289}, \"5\": {\"occluded\": 0, \"ybr\": 26, \"ytl\": 14, \"xbr\": 308, \"outside\": 1, \"attributes\": [], \"xtl\": 289}, \"6\": {\"occluded\": 0, \"ybr\": 25, \"ytl\": 14, \"xbr\": 308, \"outside\": 1, \"attributes\": [], \"xtl\": 289}, \"7\": {\"occluded\": 0, \"ybr\": 25, \"ytl\": 14, \"xbr\": 308, \"outside\": 1, \"attributes\": [], \"xtl\": 289}, \"8\": {\"occluded\": 0, \"ybr\": 25, \"ytl\": 14, \"xbr\": 307, \"outside\": 1, \"attributes\": [], \"xtl\": 289}, \"9\": {\"occluded\": 0, \"ybr\": 25, \"ytl\": 14, \"xbr\": 307, \"outside\": 1, \"attributes\": "
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Preview the annotation\n",
    "!cat $SOURCE_DATA_DIR/126_206-A0-3_json_sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79a574",
   "metadata": {},
   "source": [
    "<a name='s3.2'></a>\n",
    "### Exploratory Data Analysis ###\n",
    "We can analyze and preprocess the data using the [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). We went ahead and converted the JSON files into a .csv text file for this purpose since it's otherwise a time consuming process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2153b-d6dc-4328-bbbd-b2ec1afd1232",
   "metadata": {},
   "source": [
    "Execute the below cells to analyze the data contained in the annotation file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6184d7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the full DF object: 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes</th>\n",
       "      <th>camera</th>\n",
       "      <th>crop</th>\n",
       "      <th>frame_no</th>\n",
       "      <th>label</th>\n",
       "      <th>occluded</th>\n",
       "      <th>outside</th>\n",
       "      <th>track_id</th>\n",
       "      <th>xmax</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>ymin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>11</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>158</td>\n",
       "      <td>473</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>12</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>594</td>\n",
       "      <td>277</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>13</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>158</td>\n",
       "      <td>473</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>14</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>158</td>\n",
       "      <td>473</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>15</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>158</td>\n",
       "      <td>473</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  attributes        camera          crop  frame_no   label  occluded  outside  \\\n",
       "0     people  126_206-A0-3  (0, 0, 0, 0)        11  People         0        1   \n",
       "1     people  126_206-A0-3  (0, 0, 0, 0)        12  People         0        1   \n",
       "2     people  126_206-A0-3  (0, 0, 0, 0)        13  People         0        1   \n",
       "3     people  126_206-A0-3  (0, 0, 0, 0)        14  People         0        1   \n",
       "4     people  126_206-A0-3  (0, 0, 0, 0)        15  People         0        1   \n",
       "\n",
       "   track_id  xmax  xmin  ymax  ymin  \n",
       "0         0   305   158   473    67  \n",
       "1         0   689   594   277    62  \n",
       "2         0   305   158   473    67  \n",
       "3         0   305   158   473    67  \n",
       "4         0   305   158   473    67  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Load the .csv into a DataFrame\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "annotated_frames=pd.read_csv('data/annov2.csv', converters={2:ast.literal_eval})\n",
    "print(\"Length of the full DF object:\", len(annotated_frames))\n",
    "annotated_frames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e875a",
   "metadata": {},
   "source": [
    "We can do a `DataFrame.groupby().size()` to see how many rows there are per `frame_no`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2bad0069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frame_no\n",
       "11    1\n",
       "12    1\n",
       "13    1\n",
       "14    1\n",
       "15    1\n",
       "16    1\n",
       "17    1\n",
       "18    1\n",
       "19    1\n",
       "20    1\n",
       "21    1\n",
       "22    1\n",
       "23    1\n",
       "24    1\n",
       "25    1\n",
       "26    1\n",
       "27    1\n",
       "28    1\n",
       "29    1\n",
       "30    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check how many rows per frame_no\n",
    "annotated_frames.groupby('frame_no').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a6825",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8e74a16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the filtered DF object: 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes</th>\n",
       "      <th>camera</th>\n",
       "      <th>crop</th>\n",
       "      <th>frame_no</th>\n",
       "      <th>label</th>\n",
       "      <th>occluded</th>\n",
       "      <th>outside</th>\n",
       "      <th>track_id</th>\n",
       "      <th>xmax</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>ymin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>11</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>158</td>\n",
       "      <td>473</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>12</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>594</td>\n",
       "      <td>277</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>13</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>158</td>\n",
       "      <td>473</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>14</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>158</td>\n",
       "      <td>473</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>126_206-A0-3</td>\n",
       "      <td>(0, 0, 0, 0)</td>\n",
       "      <td>15</td>\n",
       "      <td>People</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "      <td>158</td>\n",
       "      <td>473</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  attributes        camera          crop  frame_no   label  occluded  outside  \\\n",
       "0     people  126_206-A0-3  (0, 0, 0, 0)        11  People         0        1   \n",
       "1     people  126_206-A0-3  (0, 0, 0, 0)        12  People         0        1   \n",
       "2     people  126_206-A0-3  (0, 0, 0, 0)        13  People         0        1   \n",
       "3     people  126_206-A0-3  (0, 0, 0, 0)        14  People         0        1   \n",
       "4     people  126_206-A0-3  (0, 0, 0, 0)        15  People         0        1   \n",
       "\n",
       "   track_id  xmax  xmin  ymax  ymin  \n",
       "0         0   305   158   473    67  \n",
       "1         0   689   594   277    62  \n",
       "2         0   305   158   473    67  \n",
       "3         0   305   158   473    67  \n",
       "4         0   305   158   473    67  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# include annotations that do not have person inside the bbox\n",
    "filtered_frames=annotated_frames[annotated_frames[\"outside\"] == 1]\n",
    "print(\"Length of the filtered DF object:\", len(filtered_frames))\n",
    "filtered_frames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b4ab7-832e-4985-9417-b5e49db7fa92",
   "metadata": {},
   "source": [
    "The filtered DataFrame is much smaller. We can plot the _Frame Indices that Include Moving people_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1871870f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Plot frames that include moving people\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "frames_list=list(filtered_frames['frame_no'].unique())\n",
    "frame_existance=np.zeros(annotated_frames['frame_no'].max()+1)\n",
    "for i in frames_list:\n",
    "    frame_existance[int(i)]=1\n",
    "y_pos=np.arange(len(frame_existance))\n",
    "fig, ax=plt.subplots(figsize=(18, 3))\n",
    "plt.bar(y_pos, frame_existance, align='center', alpha=0.5)\n",
    "plt.title('Frame Indices that Include Moving People')\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef45127",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d0e403c",
   "metadata": {},
   "source": [
    "<a name='s3.3'></a>\n",
    "### Convert Video File into Frame Images ###\n",
    "Because the object detection model operates on frame-based data, we will need to generate frames from the original movie file. To do so, we are going to use [OpenCV](https://opencv.org/) to open the video file and write a `.png` image file for each frame that has annotation. We will be using the original `.mp4` file at 10 FPS. In addition to converting video frames into `.png` images, we are creating a video for which the annotations are displayed as bounding boxes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03427fe3-bd0b-4c90-b1f1-6d38993a31db",
   "metadata": {},
   "source": [
    "Execute the below cells to create an annotated video and extract annotated images for the TAO Toolkit. This can take up to 5 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0dfa2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Define function to extract images and generate an annotated video\n",
    "import cv2\n",
    "colors = [(255, 255, 0), (255, 0, 255), (0, 255, 255), (0, 0, 255), (255, 0, 0), (0, 255, 0), (0, 0, 0), (255, 100, 0), (100, 255, 0), (100, 0, 255), (255, 0, 100)]\n",
    "\n",
    "def save_images(video_path, image_folder, frames_list, annotated_frames,  video_out_folder, fps=10):\n",
    "    # Create image folder if it doesn't exist\n",
    "    if not os.path.exists(image_folder):\n",
    "        print(\"Creating images folder\")\n",
    "        os.makedirs(image_folder)\n",
    "    \n",
    "    # Create directory for output video\n",
    "    if not os.path.exists(video_out_folder):\n",
    "        print(\"Creating video out folder\")\n",
    "        os.makedirs(video_out_folder)\n",
    "    \n",
    "    # Start reading input video\n",
    "    input_video=cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # cv2.VideoCapture().read() returns true if it has a next frame\n",
    "    retVal, im=input_video.read()\n",
    "    size=im.shape[1], im.shape[0]\n",
    "    fourcc=cv2.VideoWriter_fourcc('h','2','6','4') \n",
    "    \n",
    "    # Start writing output video\n",
    "    output_video=cv2.VideoWriter('{}/annotated_video.mp4'.format(video_out_folder), fourcc, fps, size)\n",
    "\n",
    "    frameCount=0\n",
    "    i=1\n",
    "    \n",
    "    # While has next frame\n",
    "    while retVal:\n",
    "        print(\"\\rProcessing frame no: {}\".format(frameCount), end='', flush=True)\n",
    "        \n",
    "        # If current frame is in the list of annotated frames, draw bounding box(es) and include in the output video\n",
    "        if frameCount in frames_list:\n",
    "            print(\"\\rSaving frame no: {}, index: {} out of {}\".format(frameCount, i, len(frames_list)), end='')\n",
    "            cv2.imwrite(os.path.join(image_folder, '{}.png'.format(frameCount)), im)\n",
    "            i+=1\n",
    "            frame_items=annotated_frames[annotated_frames[\"frame_no\"]==int(frameCount)]\n",
    "            for index, box in frame_items.iterrows():\n",
    "                xmin, ymin, xmax, ymax = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
    "                xmin2, ymin2, xmax2, ymax2 = box[\"crop\"][0], box[\"crop\"][1], box[\"crop\"][2], box[\"crop\"][3]\n",
    "                cv2.rectangle(im, (xmin, ymin), (xmax, ymax), colors[0], 1)\n",
    "                cv2.rectangle(im, (int(xmin2), int(ymin2)), (int(xmax2), int(ymax2)), colors[1], 1)\n",
    "            output_video.write(im)\n",
    "\n",
    "        # Read next frame\n",
    "        retVal, im=input_video.read()\n",
    "        frameCount+=1\n",
    "\n",
    "    input_video.release()\n",
    "    output_video.release()\n",
    "    return size        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3187abcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame no: 159ex: 20 out of 20"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(848, 480)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Extract images and generate an annotated video\n",
    "save_images('{}/VID-20220829-WA0021.mp4'.format(os.environ['SOURCE_DATA_DIR']), \n",
    "            '{}/{}'.format('{}/training'.format(os.environ['DATA_DIR']), 'images'),\n",
    "            frames_list,\n",
    "            filtered_frames,\n",
    "            '{}/{}'.format(os.environ['DATA_DIR'], 'video_out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "043185ff-923e-4fa4-ba55-346c2fe4ee5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"tao_project/data/video_out/annotated_video.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the annotated output video\n",
    "from IPython.display import Video\n",
    "Video('tao_project/data/video_out/annotated_video.mp4', width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8dc9ad",
   "metadata": {},
   "source": [
    "<a name='s3.4'></a>\n",
    "### Generate Labels ###\n",
    "We also need to generate KITTI format labels for each frame, which is also described in the [TAO Toolkit User Guide](https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html#label-files). A KITTI format label file is a simple text file containing one line per object. Each line has multiple fields. The sum of the total number of elements per object is 15 as shown below: <br>\n",
    "`class name`, `truncation`, `occlusion`, `alpha`, `xmin`, `ymin`, `xmax`, `ymax`, `height`, `weight`, `length`, `x`, `y`, `z`, `rotation_y` <br>\n",
    "Currently, for detection the TAO Toolkit only requires the class name and bbox coordinates fields to be populated. This is because the TAO Toolkit training pipe supports training only for class and bbox coordinates. The remaining fields may be set to 0 as placeholder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5df11-9c18-4a91-82a5-fa64685686a6",
   "metadata": {},
   "source": [
    "Execute the below cells to generate the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4fee05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating labels folder\n",
      "Writing for frame 14238\r"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Generate labels in KITTI format\n",
    "label_folder='{}/training/labels'.format(os.environ['DATA_DIR'])\n",
    "if not os.path.exists(label_folder):\n",
    "    print(\"Creating labels folder\")\n",
    "    os.makedirs(label_folder)\n",
    "for frame in sorted(frames_list): \n",
    "    current_frame=filtered_frames[filtered_frames['frame_no']==frame]\n",
    "    with open('{}/{}.txt'.format(label_folder, frame), 'w') as f: \n",
    "        for i, box in current_frame.iterrows(): \n",
    "            print('Writing for frame {}'.format(frame), end='\\r')\n",
    "            f.write(\"Car 0 0 0 {} {} {} {} 0 0 0 0 0 0 0\\n\".format(box['xmin'], box['ymin'], box['xmax'], box['ymax']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANAGE THIS CELL\n",
    "# Preview sample KITTI format labels\n",
    "!cat $DATA_DIR/training/labels/20.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7df4f1",
   "metadata": {},
   "source": [
    "<a name='s3.5'></a>\n",
    "### Converting to TFRecords ###\n",
    "The TAO Toolkit enables converting the training data into the [**TFRecords**](https://www.tensorflow.org/tutorials/load_data/tfrecord) format, which is a simple format for storing a sequence of binary records. The TFRecord specification encodes an image frame and all the annotations associated with that frame into a single row. This can drastically help iterate faster through the data. The TAO Toolkit helps us easily convert data to TFRecord format once it's in the KITTI format. This can be done using the `dataset_convert` subtask. The `dataset_convert` tool requires a configuration file as input, which has the below parameters: \n",
    "* `kittie_config`\n",
    "    * `root_directory_path (str)`: Path to the data set root. \n",
    "    * `image_dir_name (str)`: Relative path to the directory containing images. \n",
    "    * `label_dir_name (str)`: Relative path to the directory containing labels. \n",
    "    * `partition_mode (str)`: Method _(\"random\" or \"sequence\")_ employed when partitioning the data into folds. \n",
    "    * `num_partitions (int)`: Number of partitions (folds) to split the data _(default=2)_. This field is ignored when the partition mode is set to \"random\" as by default only two partitions are generated: `train` and `val`. \n",
    "    * `image_extension (str)`: Extension of the images _(\".png\", \".jpg\", or \".jpeg\")_. \n",
    "    * `val_split (float)`: Percentage of data to be separated for validation _(0-100)_. \n",
    "    * `num_shards (int)`: The number of shards per fold _(1-20)_. When you have large amounts of samples, it is beneficial to shard your data set into multiple files as it allows inputs to 1) be read in parallel to improve throughput and 2) be shuffled better to improve performance of the model. This is particularly important when the data set is large. You can find more information on sharding on [TensorFlow's API documentation](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset#raises_1). \n",
    "* `image_directory_path (str)`: Path to the data set root. \n",
    "\n",
    "Once generated, you can use the TFRecords across multiple training experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5bcfc",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise #2 - Dataset Convert ####\n",
    "Let's use the `dataset_convert` subtask to generate TFRecords. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the [TFRecords conversion spec file](spec_files/kitti_config.txt) by changing the `<FIXME>`s into the correct values and **save changes**. \n",
    "* Execute the below cells to create TFRecords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e32483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View the spec file\n",
    "!cat $SPEC_FILES_DIR/kitti_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a7d55-f0ff-403d-923a-c1f877a8ba2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kitti_config {\n",
    "#   root_directory_path: \"/dli/task/tao_project/data/training\"\n",
    "#   image_dir_name: \"images\"\n",
    "#   label_dir_name: \"labels\"\n",
    "#   image_extension: \".png\"\n",
    "#   partition_mode: \"random\"\n",
    "#   num_partitions: 2\n",
    "#   val_split: 20\n",
    "#   num_shards: 10\n",
    "# }\n",
    "# image_directory_path: \"/dli/task/tao_project/data/training\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c1c8d-d612-40aa-9e48-843aae048fcf",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b268d7c-d554-4daf-8ce1-ee76da69e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View dataset_convert usage\n",
    "!detectnet_v2 dataset_convert --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fce70-d634-48d6-94e1-72d5e366882a",
   "metadata": {},
   "source": [
    "When using the `dataset_convert` subtask, the `-o` argument indicates the output filename and the `-d` argument points to the path to the detection data set spec file containing the config for exporting `.tfrecord` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14481bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Create directory for TFRecords and delete existing files if they exist\n",
    "!mkdir -p $DATA_DIR/tfrecords && rm -rf $DATA_DIR/tfrecords/*\n",
    "\n",
    "!detectnet_v2 dataset_convert -d $SPEC_FILES_DIR/kitti_config.txt \\\n",
    "                              -o $DATA_DIR/tfrecords/kitti_trainval/kitti_trainval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd0daf",
   "metadata": {},
   "source": [
    "Check the shards that have been created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b2899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Check the shards that have been created\n",
    "!ls -rlt $DATA_DIR/tfrecords/kitti_trainval/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae1d3a0",
   "metadata": {},
   "source": [
    "**Well Done**! When you're ready, let's move to the [next notebook](./03_model_training_with_the_TAO_Toolkit.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f613e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
